# Emily J. Herron

Postdoctoral Research Associate at Oak Ridge National Laboratory  
Analytics and AI Methods at Scale Group  
emily.j.herron@gmail.com | [CV](#) | [Google Scholar](#) | [LinkedIn](#)

## About

I'm a Postdoctoral Research Associate at Oak Ridge National Laboratory in the Analytics and AI Methods at Scale Group. My research focuses on machine learning, particularly in neural architecture search, large language models, and AI trustworthiness. I completed my Ph.D. in Data Science & Engineering from the University of Tennessee, where I worked on generalized differentiable neural architecture search with scaling and stability improvements.

## Current Work

- Developing pipelines for assessing LLM trustworthiness in scientific applications, focusing on truthfulness, adversarial robustness, and ethics
- Incorporating evolutionary neural architecture search into mixture of experts (MoE) transformer architectures at scale
- Researching LLM-based hypothesis generation in fields of natural language processing and materials science

## Publications

**ChatHPC: Empowering HPC Users with Large Language Models**  
J. Yin et al.  
*Journal of Supercomputing, 2025*  
DOI: [https://doi.org/10.1007/s11227-024-06637-1](https://doi.org/10.1007/s11227-024-06637-1)

**Exploring Scientific Hypothesis Generation with Mamba**  
M. Chai, E. Herron, E. Cervantes, and T. Ghosal  
*Proceedings of the 1st Workshop on NLP for Science (NLP4Science), ACL, 2024*  
Pages 197-207

**SciTrust: Evaluating the Trustworthiness of Large Language Models for Science**  
E. Herron, J. Yin, and F. Wang  
*AI4S: 5th Workshop on Artificial Intelligence and Machine Learning for Scientific Applications, 2024*

**ICDARTS: Improving the Stability and Performance of Cyclic DARTS**  
E. Herron, D. Rose, and S. Young  
*arXiv, 2023*  
[https://arxiv.org/abs/2309.00664](https://arxiv.org/abs/2309.00664)

**Ensembles of Networks Produced from Neural Architecture Search**  
E. J. Herron, S. R. Young, and T. E. Potok  
*International Conference on High Performance Computing, 2020*  
Pages 223-234

## Education

**Ph.D. in Data Science & Engineering**  
University of Tennessee, Knoxville  
2018 - 2023
- Thesis: Generalized Differentiable Neural Architecture Search with Scaling and Stability Improvements
- Advisor: Dr. Steven R. Young
- GPA: 3.95 / 4.00

**B.S. in Computational Science**  
Mercer University  
2014 - 2018
- Graduated Summa Cum Laude
- GPA: 3.94 / 4.00

## Experience

**Oak Ridge National Laboratory**  
*Postdoctoral Research Associate* (Jan 2024 - Present)
- Developing pipeline for assessing the trustworthiness of large language models for science
- Incorporating evolutionary neural architecture search into mixture of experts (MoE) transformer architectures
- Researching LLM-based hypothesis generation in NLP and materials science

*Graduate Research Assistant* (Aug 2018 - Dec 2023)
- Developed stability improvements to CDARTS NAS algorithm
- Researched and implemented selection algorithms for ORNL's MENNDL NAS software
- Collaborated on creation of NIEHS document mining pipeline

## Professional Service

- ORNL Traveling Science Fair Volunteer (2024)
- ORNL Undergraduate Summer Internship Program Mentor (2024)
- Assistant Instructor, ORNLAI Summer Institute Tutorial (2024)
- ICML 2022 Reviewer - Top 10%
- Bredesen Center Peer Mentor (2022-2023)

## Awards & Honors

- OLCF Director's Discretion Project - 20,000 Summit Hours (2022-2023)
- Bredesen Center Data Science & Engineering Fellowship (2018)
- ACM Student Poster Competition Undergraduate Semifinalist (2017)
- Outstanding Student in Computational Science, Mercer University (2016-2018)
- Summa Cum Laude, Mercer University (2018)

## Professional Affiliations

- Institute of Electrical and Electronics Engineers (IEEE) Member (2022 - Present)
- Association for Computing Machinery (ACM) Member (2017 - Present)

